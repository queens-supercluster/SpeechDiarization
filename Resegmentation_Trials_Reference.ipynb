{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8CU1wxPQPW8baS47Zwotb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/queens-supercluster/SpeechDiarization/blob/main/Resegmentation_Trials_Reference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RESEGMENTATION"
      ],
      "metadata": {
        "id": "IeUIPXid-Vfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Refine speaker labels using Variational Bayesian PLDA\n",
        "\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "\n",
        "plda = BayesianGaussianMixture(n_components=num_speakers, covariance_type='diag', weight_concentration_prior_type='dirichlet_process').fit(embeddings)\n",
        "\n",
        "refined_labels = np.zeros_like(labels)\n",
        "for i in range(len(segments)):\n",
        "    segment_embeddings = embeddings[i].reshape(1, -1)\n",
        "    scores = plda.score_samples(segment_embeddings)[0]\n",
        "    refined_labels[i] = np.argmax(scores)\n",
        "\n",
        "# Update segment speakers with refined labels\n",
        "for i in range(len(segments)):\n",
        "    segments[i][\"speaker\"] = 'SPEAKER ' + str(refined_labels[i] + 1)\n",
        "\n",
        "# Write transcript with refined segmentation\n",
        "with open(\"transcript3.txt\", \"w\") as f:\n",
        "    for (i, segment) in enumerate(segments):\n",
        "        if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "            f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
        "        f.write(segment[\"text\"][1:] + ' ')"
      ],
      "metadata": {
        "id": "ms6ef_IV-VXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2nd Instance\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "def diarization_refinement(embeddings, labels):\n",
        "    num_clusters = len(np.unique(labels))\n",
        "\n",
        "    # Initialize GMM-HMM models for each cluster\n",
        "    models = []\n",
        "    for _ in range(num_clusters):\n",
        "        gmm = GaussianHMM(num_speakers, covariance_type='diag',n_iter= 1000)\n",
        "        models.append(gmm)\n",
        "\n",
        "    # Iterate over each cluster and re-estimate the GMM\n",
        "    for cluster_id in range(num_clusters):\n",
        "        cluster_embeddings = embeddings[labels == cluster_id]\n",
        "        models[cluster_id].fit(cluster_embeddings)\n",
        "\n",
        "    # Perform Viterbi re-segmentation\n",
        "    viterbi_segments = []\n",
        "    for embedding in embeddings:\n",
        "        scores = [model.score(embedding.reshape(1, -1)) for model in models]\n",
        "        predicted_cluster = np.argmax(scores)\n",
        "        viterbi_segments.append(predicted_cluster)\n",
        "\n",
        "    return viterbi_segments\n",
        "\n",
        "refined_labels = diarization_refinement(embeddings, labels)\n",
        "\n",
        "# Assign refined labels to segments\n",
        "for i in range(len(segments)):\n",
        "    segments[i][\"speaker\"] = 'SPEAKER ' + str(refined_labels[i] + 1)"
      ],
      "metadata": {
        "id": "Xswls8Vzvsis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd instance\n",
        "\n",
        "import numpy as np\n",
        "from hmmlearn.hmm import GaussianHMM\n",
        "\n",
        "def resegmentation(initial_segments, embeddings):\n",
        "    # Extracting initial cluster assignments from the initial segments\n",
        "    initial_clusters = [segment['speaker'] for segment in initial_segments]\n",
        "    unique_speakers = list(set(initial_clusters))  #trying to make code more dynamic - to remove num_speakers\n",
        "\n",
        "    # Determining mumber of components for HMM\n",
        "    n_components = len(unique_speakers)\n",
        "\n",
        "    # Preparing features for HMM training\n",
        "    features = embeddings.reshape(len(embeddings), -1)  # Reshape embeddings if needed\n",
        "\n",
        "    hmm = GaussianHMM(n_components, covariance_type='diag', n_iter= 100)\n",
        "    hmm.fit(features)\n",
        "\n",
        "    #log_likelihoods, _ = hmm.score_samples(features)\n",
        "\n",
        "    resegmented_segments = initial_segments.copy()\n",
        "\n",
        "    #Viterbi re-segmentation\n",
        "    n_frames = len(features)\n",
        "    state_sequence = hmm.predict(features)  # Viterbi decoding\n",
        "\n",
        "    # re-segmented segments based on the new state sequence\n",
        "    for i, segment in enumerate(resegmented_segments):\n",
        "        segment['speaker'] = 'SPEAKER ' + str(state_sequence[i] + 1)\n",
        "\n",
        "    return resegmented_segments\n",
        "\n",
        "resegmented_segments = resegmentation(segments, embeddings)\n"
      ],
      "metadata": {
        "id": "0dXRPrwTrJ61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4th Instance (ChatGPT3)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from hmmlearn import hmm\n",
        "\n",
        "n_states = 2  # Speaker A, Speaker B, Non-speech\n",
        "models = [hmm.GaussianHMM(n_components=n_states) for _ in range(2)]  # Two speakers\n",
        "non_speech_model = hmm.GaussianHMM(n_components=n_states)\n",
        "\n",
        "def calculate_posterior_probabilities(embeddings):\n",
        "    # Define HMM parameters\n",
        "    n_states = 2  # Number of HMM states (Speaker A, Speaker B, non-speech N)\n",
        "    n_features = embeddings.shape[1]  # Number of features in the embeddings\n",
        "\n",
        "    # Initialize the HMM model\n",
        "    model = hmm.GaussianHMM(n_components=n_states)\n",
        "\n",
        "    # Train the HMM model on the embeddings\n",
        "    model.fit(embeddings)\n",
        "\n",
        "    # Compute the posterior probabilities for each embedding using the trained HMM model\n",
        "    posterior_probabilities = model.predict_proba(embeddings)\n",
        "\n",
        "    return posterior_probabilities\n",
        "\n",
        "# Baum-Welch training\n",
        "for i, model in enumerate(models):\n",
        "    # Get embeddings and posterior probabilities for the current speaker\n",
        "    speaker_embeddings = embeddings[labels == i]\n",
        "    speaker_posteriors = calculate_posterior_probabilities(speaker_embeddings)  # Replace with your calculation\n",
        "\n",
        "    # Train the speaker model using Baum-Welch\n",
        "    model.fit(speaker_embeddings, lengths=speaker_posteriors.shape[0])\n",
        "\n",
        "# Viterbi re-segmentation\n",
        "max_iterations = 20\n",
        "for _ in range(max_iterations):\n",
        "    # Initialize statistics for Baum-Welch re-estimation\n",
        "    total_posteriors = np.zeros(n_states)\n",
        "    total_features = np.zeros((n_states, embeddings.shape[1]))\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        # Calculate posterior probabilities for each embedding\n",
        "        speaker_posteriors = model.predict_proba(embeddings)\n",
        "\n",
        "        # Accumulate statistics for re-estimation\n",
        "        total_posteriors += np.sum(speaker_posteriors, axis=0)\n",
        "        total_features += np.dot(speaker_posteriors.T, embeddings)\n",
        "\n",
        "    # Update models using Baum-Welch re-estimation\n",
        "    for i, model in enumerate(models):\n",
        "        model.startprob_ = total_posteriors / np.sum(total_posteriors)\n",
        "        model.means_ = total_features / total_posteriors[:, np.newaxis]\n",
        "\n",
        "    # Assign frames to the model with the highest posterior probability\n",
        "    new_labels = np.argmax(np.concatenate([model.predict_proba(embeddings) for model in models]), axis=1)\n",
        "\n",
        "    # Check for convergence\n",
        "    if np.array_equal(new_labels, labels):\n",
        "        break\n",
        "\n",
        "    labels = new_labels\n",
        "\n",
        "# 4.2.4 Second Pass Refinements\n",
        "\n",
        "# Initialize embeddings for each speaker\n",
        "embeddings_A = np.zeros((np.sum(labels == 0), embeddings.shape[1]))\n",
        "embeddings_B = np.zeros((np.sum(labels == 1), embeddings.shape[1]))\n",
        "\n",
        "# Assign embeddings to the corresponding speaker\n",
        "index_A = 0\n",
        "index_B = 0\n",
        "for i, label in enumerate(labels):\n",
        "    if label == 0:\n",
        "        embeddings_A[index_A] = embeddings[i]\n",
        "        index_A += 1\n",
        "    else:\n",
        "        embeddings_B[index_B] = embeddings[i]\n",
        "        index_B += 1\n",
        "\n",
        "# Perform K-means clustering on embeddings to reassign segments\n",
        "kmeans_A = KMeans(n_clusters=2)\n",
        "labels_A = kmeans_A.fit_predict(embeddings_A)\n",
        "\n",
        "kmeans_B = KMeans(n_clusters=2)\n",
        "labels_B = kmeans_B.fit_predict(embeddings_B)\n",
        "\n",
        "# Update segment assignments based on new clustering\n",
        "new_labels = np.zeros_like(labels)\n",
        "for i, label in enumerate(labels):\n",
        "    if label == 0:\n",
        "        new_labels[i] = labels_A[index_A]\n",
        "        index_A += 1\n",
        "    else:\n",
        "        new_labels[i] = labels_B[index_B]\n",
        "        index_B += 1\n",
        "\n",
        "# Check for convergence\n",
        "if np.array_equal(new_labels, labels):\n",
        "    break\n",
        "\n",
        "labels = new_labels\n",
        "\n",
        "# Continue the refinement process until convergence\n",
        "\n",
        "# Helper function to calculate posterior probabilities\n",
        "def calculate_posterior_probabilities(embeddings):\n",
        "    # Replace with your implementation to calculate posterior probabilities\n",
        "    return np.random.rand(embeddings.shape[0], n_states)\n"
      ],
      "metadata": {
        "id": "Ue9PgF7PoJJR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6685f632-8981-471d-8814-0ab7422771e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Model is not converging.  Current: 10908.285072011511 is not greater than 10908.303628836718. Delta is -0.018556825207269867\n",
            "WARNING:hmmlearn.base:Model is not converging.  Current: 10707.528636342395 is not greater than 10708.771574733892. Delta is -1.242938391496864\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8bf71c6af698>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0membeddings_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_A\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mindex_A\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 69 is out of bounds for axis 0 with size 69"
          ]
        }
      ]
    }
  ]
}